{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torchvision import datasets, transforms, models\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "aPB8wduah2hE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TOIDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = torch.tensor(x, dtype=torch.float32)\n",
        "    self.y = torch.tensor(y.values, dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "TE3HlTGwi2hu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "nSN8KpfLREQU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSktufUTRiss",
        "outputId": "10bc7e6b-95e9-4ae7-c4ad-43c0b5201232"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/new_combined.csv').dropna()\n",
        "\n",
        "y = data.pop('disp').astype('float64')\n",
        "X = data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                    test_size=0.05,\n",
        "                                    random_state=1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                    test_size=0.1,\n",
        "                                    random_state=1)\n",
        "X_final_test = X_test\n",
        "y_final_test = y_test\n",
        "features_num = [\n",
        "    'ra', 'dec', 'orbper', 'duration', 'depth', 'rad', 'steff',\n",
        "    'logg', 'srad'\n",
        "]\n",
        "features_cat = []\n",
        "\n",
        "transformer_num = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    StandardScaler(),\n",
        ")\n",
        "\n",
        "transformer_cat = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\", fill_value=\"NA\"),\n",
        "    OneHotEncoder(handle_unknown='ignore'),\n",
        ")\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (transformer_num, features_num),\n",
        "    (transformer_cat, features_cat),\n",
        ")\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_val = preprocessor.transform(X_val)\n",
        "X_test = preprocessor.transform(X_test)\n"
      ],
      "metadata": {
        "id": "02RbykY-Bb6W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9htpllQh37lx",
        "outputId": "99a2ee37-dc17-4a8f-864a-38bc503a2e20"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13572, 9), (1509, 9), (794, 9))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(y_train.unique())\n",
        "num_classes"
      ],
      "metadata": {
        "id": "VASRgGVtwPgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb74f937-f9c2-46dc-816c-876d27a5ca7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XcPCNqzuDXU",
        "outputId": "87f05e0d-48cc-4c10-e978-eff96e4251f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_comb = TOIDataset(X_train, y_train)\n",
        "val_dataset_comb = TOIDataset(X_val, y_val)"
      ],
      "metadata": {
        "id": "UN4G1SSilVHn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset_comb, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset_comb, batch_size=128)"
      ],
      "metadata": {
        "id": "tkwt-jrqluy9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(y_train.unique())\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy4L5X6ljTle",
        "outputId": "cb669859-8928-4ab3-c04c-eed2c95cf544"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {'lr': 0.0006056915778868701, 'batch_size': 128, 'out_channels1': 64, 'out_channels2': 512, 'kernel_size': 3, 'dropout': 0.21777348000182786, 'num_layers': 5, 'hidden_size': 512}\n",
        "class CNN1D_LSTM(nn.Module):\n",
        "    def __init__(self, num_classes=num_classes, input_features=9, out_channels1=64, out_channels2=512,\n",
        "                 kernel_size=3, dropout=0.21777348000182786, num_layers=5, hidden_size=512):\n",
        "        super(CNN1D_LSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1, out_channels=out_channels1, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(out_channels1),\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=out_channels1, out_channels=out_channels2, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(out_channels2),\n",
        "            nn.MaxPool1d(2)\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, input_features)\n",
        "            dummy = self.conv_block1(dummy)\n",
        "            dummy = self.conv_block2(dummy)\n",
        "            flattened_size = dummy.numel()\n",
        "\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "          x = x.unsqueeze(1)\n",
        "          x = self.conv_block1(x)\n",
        "          x = self.conv_block2(x)\n",
        "          x = torch.flatten(x, 1)\n",
        "          x = self.fc_block(x)\n",
        "          return x"
      ],
      "metadata": {
        "id": "JykBs4jgjc2e"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss == None:\n",
        "            self.best_loss = val_loss\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        elif self.best_loss - val_loss < self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ],
      "metadata": {
        "id": "h0QPbU67f44E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN1D_LSTM().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0006056915778868701, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.001)"
      ],
      "metadata": {
        "id": "M52GeFoEl9cd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "num_epochs = 50\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for frames, labels in train_loop:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loop.set_postfix(loss=loss.item(), accuracy=100 * train_correct / train_total)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_loader:\n",
        "            frames, labels = frames.to(device), labels.to(device)\n",
        "            with autocast():\n",
        "                outputs = model(frames)\n",
        "                loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    lr = scheduler.get_last_lr()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    early_stopping(val_loss)\n",
        "    if early_stopping.early_stop:\n",
        "      print('Early stopping')\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "f9VWMx-GjGGB",
        "outputId": "ddf6df15-e56b-415e-8e09-44fff1df298d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3272275061.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/50:   0%|          | 0/107 [00:00<?, ?it/s]/tmp/ipython-input-3272275061.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Epoch 1/50:  87%|████████▋ | 93/107 [00:04<00:00, 19.20it/s, accuracy=69.8, loss=0.507]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3272275061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss'),\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "adG3gwszmo9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = \"CNN1D_LSTM.pth\"\n",
        "\n",
        "# torch.save({\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict()\n",
        "# }, model_path)\n",
        "# print(f\"Модель збережена у {model_path}\")"
      ],
      "metadata": {
        "id": "BMcYyCrn0AAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    outputs = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "qC1u04RKp8HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "acc: 0.7909319899244333 F1-score: 0.8316430020283976"
      ],
      "metadata": {
        "id": "1RZMHYgBjAxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "c2AqGe4thplt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import optuna.visualization as vis\n",
        "# import optuna\n",
        "\n",
        "# def train_and_validate_model(model, train_loader, val_loader, num_epochs, learning_rate, patience):\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "#     scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
        "#     early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
        "\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     train_accuracies = []\n",
        "#     val_accuracies = []\n",
        "#     val_f1_scores = []\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         train_correct = 0\n",
        "#         train_total = 0\n",
        "\n",
        "#         train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "#         for frames, labels in train_loop:\n",
        "#             frames, labels = frames.to(device), labels.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             outputs = model(frames)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item()\n",
        "\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             train_total += labels.size(0)\n",
        "#             train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#             train_loop.set_postfix(loss=loss.item(), accuracy=100 * train_correct / train_total)\n",
        "\n",
        "#         train_loss = running_loss / len(train_loader)\n",
        "#         train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "#         train_losses.append(train_loss)\n",
        "#         train_accuracies.append(train_accuracy)\n",
        "\n",
        "#         model.eval()\n",
        "#         val_loss = 0.0\n",
        "#         val_correct = 0\n",
        "#         val_total = 0\n",
        "#         all_preds = []\n",
        "#         all_labels = []\n",
        "#         with torch.no_grad():\n",
        "#             for frames, labels in val_loader:\n",
        "#                 frames, labels = frames.to(device), labels.to(device)\n",
        "#                 outputs = model(frames)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#                 val_loss += loss.item()\n",
        "#                 _, predicted = torch.max(outputs, 1)\n",
        "#                 val_total += labels.size(0)\n",
        "#                 val_correct += (predicted == labels).sum().item()\n",
        "#                 all_preds.extend(predicted.cpu().numpy())\n",
        "#                 all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "#         val_loss /= len(val_loader)\n",
        "#         val_accuracy = 100 * val_correct / val_total\n",
        "#         val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "#         val_losses.append(val_loss)\n",
        "#         val_accuracies.append(val_accuracy)\n",
        "#         val_f1_scores.append(val_f1)\n",
        "\n",
        "#         scheduler.step(val_f1)\n",
        "\n",
        "#         print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "#               f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
        "#               f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "#         early_stopping(val_f1)\n",
        "#         if early_stopping.early_stop:\n",
        "#             print('Early stopping')\n",
        "#             break\n",
        "\n",
        "#     history = {\n",
        "#         'train_loss': train_losses,\n",
        "#         'val_loss': val_losses,\n",
        "#         'train_accuracy': train_accuracies,\n",
        "#         'val_accuracy': val_accuracies,\n",
        "#         'val_f1': val_f1_scores\n",
        "#     }\n",
        "#     return history\n",
        "\n",
        "# def objective(trial):\n",
        "#     lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
        "#     batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512, 1024])\n",
        "#     out_channels1 = trial.suggest_categorical(\"out_channels1\", [64, 128, 256, 512])\n",
        "#     out_channels2 = trial.suggest_categorical(\"out_channels2\", [128, 256, 512, 1024])\n",
        "#     kernel_size = trial.suggest_int(\"kernel_size\", 3, 7, step=2)\n",
        "#     dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "#     num_layer = trial.suggest_categorical(\"num_layers\", [3, 4, 5, 6, 7])\n",
        "#     hidden_size = trial.suggest_categorical(\"hidden_size\", [128, 256, 512, 1024])\n",
        "\n",
        "\n",
        "#     model = CNN1D_LSTM(num_classes=2, input_features=9,\n",
        "#                   out_channels1=out_channels1, out_channels2=out_channels2,\n",
        "#                   kernel_size=kernel_size, dropout=dropout, num_layers=num_layer, hidden_size=hidden_size).to(device)\n",
        "\n",
        "#     history = train_and_validate_model(\n",
        "#         model=model,\n",
        "#         train_loader=DataLoader(train_dataset_comb, batch_size=batch_size, shuffle=True),\n",
        "#         val_loader=DataLoader(val_dataset_comb, batch_size=batch_size, shuffle=False),\n",
        "#         num_epochs=20,\n",
        "#         learning_rate=lr,\n",
        "#         patience=5\n",
        "#     )\n",
        "\n",
        "#     best_val_f1 = max(history['val_f1'])\n",
        "#     return best_val_f1\n",
        "\n",
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# study.optimize(objective, n_trials=40)\n",
        "\n",
        "# print(\"Best parameters:\", study.best_params)\n",
        "# print(\"Best val_f1:\", study.best_value)\n",
        "\n",
        "# vis.plot_optimization_history(study).show()\n",
        "# vis.plot_param_importances(study).show()"
      ],
      "metadata": {
        "id": "n_Sc_ubrhc7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import TensorDataset, DataLoader\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# X_non_test = np.concatenate((X_train, X_val), axis=0)\n",
        "# y_non_test = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "# y_non_test = y_non_test.astype(np.int64)\n",
        "\n",
        "# k_folds = 5\n",
        "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
        "\n",
        "# fold_val_losses = []\n",
        "# fold_val_accuracies = []\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_non_test)):\n",
        "#     print(f\"Fold {fold + 1}/{k_folds}\")\n",
        "\n",
        "#     X_fold_train = X_non_test[train_idx]\n",
        "#     y_fold_train = y_non_test[train_idx]\n",
        "#     X_fold_val = X_non_test[val_idx]\n",
        "#     y_fold_val = y_non_test[val_idx]\n",
        "\n",
        "#     train_dataset = TensorDataset(torch.from_numpy(X_fold_train).float(), torch.from_numpy(y_fold_train).long())\n",
        "#     val_dataset = TensorDataset(torch.from_numpy(X_fold_val).float(), torch.from_numpy(y_fold_val).long())\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "#     model = CNN1D_LSTM()\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "#     scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
        "#     early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n",
        "#     scaler = GradScaler()\n",
        "#     num_epochs = 50\n",
        "\n",
        "#     train_losses = []\n",
        "#     val_losses = []\n",
        "#     train_accuracies = []\n",
        "#     val_accuracies = []\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()\n",
        "#     running_loss = 0.0\n",
        "#     train_correct = 0\n",
        "#     train_total = 0\n",
        "\n",
        "#     train_loop = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "#     for frames, labels in train_loop:\n",
        "#         frames, labels = frames.to(device), labels.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         with autocast():\n",
        "#             outputs = model(frames)\n",
        "#             loss = criterion(outputs, labels)\n",
        "\n",
        "#         scaler.scale(loss).backward()\n",
        "#         scaler.step(optimizer)\n",
        "#         scaler.update()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         train_total += labels.size(0)\n",
        "#         train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#         train_loop.set_postfix(loss=loss.item(), accuracy=100 * train_correct / train_total)\n",
        "\n",
        "#     train_loss = running_loss / len(train_loader)\n",
        "#     train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "#     train_losses.append(train_loss)\n",
        "#     train_accuracies.append(train_accuracy)\n",
        "\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     model.eval()\n",
        "#     val_loss = 0.0\n",
        "#     val_correct = 0\n",
        "#     val_total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for frames, labels in val_loader:\n",
        "#             frames, labels = frames.to(device), labels.to(device)\n",
        "#             with autocast():\n",
        "#                 outputs = model(frames)\n",
        "#                 loss = criterion(outputs, labels)\n",
        "#             val_loss += loss.item()\n",
        "#             _, predicted = torch.max(outputs, 1)\n",
        "#             val_total += labels.size(0)\n",
        "#             val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     val_loss /= len(val_loader)\n",
        "#     val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "#     val_losses.append(val_loss)\n",
        "#     val_accuracies.append(val_accuracy)\n",
        "\n",
        "\n",
        "#     scheduler.step(val_loss)\n",
        "#     lr = scheduler.get_last_lr()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
        "#           f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
        "#           f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "#     early_stopping(val_loss)\n",
        "#     if early_stopping.early_stop:\n",
        "#       print('Early stopping')\n",
        "#       break\n",
        "\n",
        "#     fold_val_losses.append(val_losses[-1])\n",
        "#     fold_val_accuracies.append(val_accuracies[-1])\n",
        "\n",
        "# avg_val_loss = np.mean(fold_val_losses)\n",
        "# avg_val_accuracy = np.mean(fold_val_accuracies)\n",
        "\n",
        "# print(f\"Average Val Loss across {k_folds} folds: {avg_val_loss:.4f}\")\n",
        "# print(f\"Average Val Accuracy across {k_folds} folds: {avg_val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "ILnTaheuwKVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_final_test[:1]"
      ],
      "metadata": {
        "id": "Dl1sWcVUJt78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_final_test"
      ],
      "metadata": {
        "id": "17yzbYHiMOI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # {'lr': 0.0006056915778868701, 'batch_size': 128, 'out_channels1': 64, 'out_channels2': 512, 'kernel_size': 3, 'dropout': 0.21777348000182786, 'num_layers': 5, 'hidden_size': 512}\n",
        "# class CNN1D_LSTM(nn.Module):\n",
        "#     def __init__(self, num_classes=2, input_features=9, out_channels1=64, out_channels2=512,\n",
        "#                  kernel_size=3, dropout=0.21777348000182786, num_layers=5, hidden_size=512):\n",
        "#         super(CNN1D_LSTM, self).__init__()\n",
        "#         self.lstm = nn.LSTM(input_size=input_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "#         self.conv_block1 = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels=1, out_channels=out_channels1, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm1d(out_channels1),\n",
        "#             nn.MaxPool1d(2)\n",
        "#         )\n",
        "#         self.conv_block2 = nn.Sequential(\n",
        "#             nn.Conv1d(in_channels=out_channels1, out_channels=out_channels2, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm1d(out_channels2),\n",
        "#             nn.MaxPool1d(2)\n",
        "#         )\n",
        "#         with torch.no_grad():\n",
        "#             dummy = torch.zeros(1, 1, input_features)\n",
        "#             dummy = self.conv_block1(dummy)\n",
        "#             dummy = self.conv_block2(dummy)\n",
        "#             flattened_size = dummy.numel()\n",
        "\n",
        "#         self.fc_block = nn.Sequential(\n",
        "#             nn.Linear(flattened_size, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#           x = x.unsqueeze(1)\n",
        "#           x = self.conv_block1(x)\n",
        "#           x = self.conv_block2(x)\n",
        "#           x = torch.flatten(x, 1)\n",
        "#           x = self.fc_block(x)\n",
        "#           return x\n",
        "\n",
        "# model = CNN1D_LSTM()\n",
        "\n",
        "# features_num = [\n",
        "#     'ra', 'dec', 'orbper', 'duration', 'depth', 'rad', 'steff',\n",
        "#     'logg', 'srad'\n",
        "# ]\n",
        "# features_cat = []\n",
        "\n",
        "# transformer_num = make_pipeline(\n",
        "#     SimpleImputer(strategy=\"median\"),\n",
        "#     StandardScaler(),\n",
        "# )\n",
        "\n",
        "# transformer_cat = make_pipeline(\n",
        "#     SimpleImputer(strategy=\"median\", fill_value=\"NA\"),\n",
        "#     OneHotEncoder(handle_unknown='ignore'),\n",
        "# )\n",
        "\n",
        "# preprocessor = make_column_transformer(\n",
        "#     (transformer_num, features_num),\n",
        "#     (transformer_cat, features_cat),\n",
        "# )\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Z = pd.read_csv('/content/new_combined.csv')\n",
        "# y_f = Z.pop('disp')\n",
        "# # Z = preprocessor.transform(Z)\n",
        "\n",
        "# state_dict = torch.load('/content/CNN1D_LSTM.pth')\n",
        "\n",
        "# model.load_state_dict(state_dict['model_state_dict'])\n",
        "# model = model.to(device)\n",
        "\n",
        "# sample_idx = range(len(Z))\n",
        "# sample_idx\n",
        "\n",
        "# for i in sample_idx:\n",
        "#     sample_data = Z.iloc[i]\n",
        "#     # true_label = y_final_test.iloc[i]\n",
        "\n",
        "#     sample_processed = preprocessor.transform(pd.DataFrame(sample_data).T)\n",
        "#     sample_tensor = torch.tensor(sample_processed, dtype=torch.float32).to(device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         output = model(sample_tensor)\n",
        "#         _, predicted = torch.max(output, 1)\n",
        "#         predicted_label = predicted.item()\n",
        "\n",
        "#     # print(f\"True class: {true_label}\")\n",
        "#     print(f\"Predicted class: {predicted_label}\")\n",
        "#     logits = output\n",
        "\n",
        "#     probs = torch.softmax(logits, dim=1)\n",
        "#     print(\"Ймовірності:\", probs.cpu().numpy())\n",
        "\n",
        "#     predicted_class = torch.argmax(logits, dim=1).item()\n",
        "#     print(\"Кінцевий результат (клас):\", predicted_class, '\\n')"
      ],
      "metadata": {
        "id": "4yIwJGyxgo6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = [1, 2, 3, 4, 5]\n",
        "\n",
        "for i in sample_idx:\n",
        "    sample_data = X_final_test.iloc[i]\n",
        "    true_label = y_final_test.iloc[i]\n",
        "\n",
        "    sample_processed = preprocessor.transform(pd.DataFrame(sample_data).T)\n",
        "    sample_tensor = torch.tensor(sample_processed, dtype=torch.float32).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(sample_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        predicted_label = predicted.item()\n",
        "\n",
        "    print(\"Інформація про об'єкт:\")\n",
        "    print(sample_data, '\\n')\n",
        "    print(f\"True class: {true_label}\")\n",
        "    print(f\"Predicted class: {predicted_label}\")\n",
        "    logits = output\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    print(\"Ймовірності:\", probs.cpu().numpy())\n",
        "\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "    print(\"Кінцевий результат (клас):\", predicted_class, '\\n')"
      ],
      "metadata": {
        "id": "nu_SWuhTMb7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MACHINE LEARNING**"
      ],
      "metadata": {
        "id": "Wmio0wULBwnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "OuoLX0TppFWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scipy import stats\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.ensemble import StackingClassifier\n"
      ],
      "metadata": {
        "id": "xBmErmLFCTzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "u8Mqn8vNUhjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf = SVC(kernel = 'rbf', C = 10, gamma = 0.5)\n",
        "\n",
        "# params = {\"C\":(0.1, 0.5, 1, 2, 5, 10),\n",
        "#           \"gamma\":(0.01, 0.1, 0.5, 1),}\n",
        "\n",
        "# svm_cv = GridSearchCV(svm_clf, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")\n",
        "# svm_cv.fit(X_train, y_train)\n",
        "# best_params = svm_cv.best_params_\n",
        "# print(f\"Best params: {best_params}\")\n",
        "\n",
        "# svm_clf = SVC(**best_params)\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "l-uhK7IhXECw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = tree_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "svtCm77hX0df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best paramters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': 0.5, 'max_depth': None, 'criterion': 'entropy', 'class_weight': 'balanced', 'bootstrap': True}\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# param_dist = {\n",
        "#         'n_estimators': [50, 100, 200],\n",
        "#         'max_depth': [None, 10, 20],\n",
        "#         'min_samples_split': [2, 5, 10],\n",
        "#         'min_samples_leaf': [1, 2, 4],\n",
        "#         'max_features': ['sqrt', 'log2'],\n",
        "#         'bootstrap': [True, False]\n",
        "#     }\n",
        "\n",
        "# rf_clf = RandomForestClassifier()\n",
        "# rf_clf = RandomizedSearchCV(\n",
        "#     rf_clf, param_dist, n_iter=15,\n",
        "#     scoring='accuracy', n_jobs=-1, verbose=1\n",
        "# )\n",
        "\n",
        "# rf_clf.fit(X_train, y_train)\n",
        "# best_params = rf_clf.best_params_\n",
        "# print(f\"Best paramters: {best_params}\")\n",
        "\n",
        "# rf_clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "S0ZctaBVYBHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# {'learning_rate': np.float64(0.1530530058619579), 'max_depth': 9, 'n_estimators': 227} 0.8229166666666666\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, learning_rate = np.float64(0.0666175730453532), max_depth = 6, n_estimators = 394)\n",
        "\n",
        "# param_grid = dict(\n",
        "#     n_estimators=stats.randint(10, 1000),\n",
        "#     max_depth=stats.randint(1, 10),\n",
        "#     learning_rate=stats.uniform(0, 1)\n",
        "# )\n",
        "\n",
        "# xgb_clf = XGBClassifier(use_label_encoder=False)\n",
        "# xgb_cv = RandomizedSearchCV(\n",
        "#     xgb_clf, param_grid, n_iter=45,\n",
        "#     scoring='accuracy', n_jobs=-1, verbose=1\n",
        "# )\n",
        "# xgb_cv.fit(X_train, y_train)\n",
        "# best_params = xgb_cv.best_params_\n",
        "# print(f\"Best paramters: {best_params}\")\n",
        "\n",
        "# xgb_clf = XGBClassifier(**best_params)\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHmB7ixZYJnz",
        "outputId": "a18d2c1d-b9ad-4e72-c8e2-0c643cb3dfe9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [19:39:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7846347607052897\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.67      0.72       335\n",
            "         1.0       0.78      0.87      0.82       459\n",
            "\n",
            "    accuracy                           0.78       794\n",
            "   macro avg       0.79      0.77      0.77       794\n",
            "weighted avg       0.79      0.78      0.78       794\n",
            "\n",
            "F1-score: 0.8235294117647058, 0.7814301933320582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm = LGBMClassifier(n_estimators=500, max_depth=8, learning_rate=0.1, num_leaves=31, random_state=42)\n",
        "\n",
        "lgbm.fit(X_train, y_train)\n",
        "y_pred = lgbm.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "2Nu8obmRaZHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost = CatBoostClassifier(iterations=500, depth=8, learning_rate=0.1, random_seed=42,verbose=False)\n",
        "\n",
        "catboost.fit(X_train, y_train)\n",
        "y_pred = catboost.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "_Z4q2LHZbcXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extratrees = ExtraTreesClassifier(n_estimators=500, max_depth=15, min_samples_split=5, random_state=42)\n",
        "\n",
        "extratrees.fit(X_train, y_train)\n",
        "y_pred = extratrees.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "MEp6LPtabch6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=8)\n",
        "\n",
        "gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gb.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "srT6wIqps26D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(gb, X, y, cv=5).mean())"
      ],
      "metadata": {
        "id": "KPIwFGpYt66b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "xgb = XGBClassifier(use_label_encoder=False, learning_rate = np.float64(0.0666175730453532), max_depth = 6, n_estimators = 394, random_state=42)\n",
        "svm_clf = SVC(kernel = 'rbf', C = 10, gamma = 0.5, random_state=42)\n",
        "\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf),\n",
        "            ('xgb', xgb),\n",
        "            ('svm_clf', svm_clf)\n",
        "        ], voting='hard'\n",
        "    )\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1-score: {f1_score(y_test, y_pred)}, {f1_score(y_test, y_pred,average='weighted')}\")"
      ],
      "metadata": {
        "id": "ZjtSebzMziSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(rf, X, y, cv=5).mean()),\n",
        "print(cross_val_score(xgb, X, y, cv=5).mean()),\n",
        "print(cross_val_score(catboost, X, y, cv=5).mean()),\n",
        "print(cross_val_score(lgbm, X, y, cv=5).mean())"
      ],
      "metadata": {
        "id": "ldiHheJX5_Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(voting_clf, X, y, cv=5).mean())"
      ],
      "metadata": {
        "id": "Tx8XRBjV1rQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_probs = xgb_clf.predict_proba(X_test)\n",
        "xgb_probs = xgb_probs[:, 1]\n",
        "\n",
        "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
        "\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(xgb_fpr, xgb_tpr, linestyle='--', label='(AUC = %0.3f)' % xgb_auc)\n",
        "\n",
        "plt.title('ROC Plot')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_F1rY3Ma0l2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')"
      ],
      "metadata": {
        "id": "7KURZYSZ5hDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_idx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "for i in sample_idx:\n",
        "    sample_data = X_final_test.iloc[i]\n",
        "    true_label = y_final_test.iloc[i]\n",
        "\n",
        "    sample_processed = preprocessor.transform(pd.DataFrame(sample_data).T)\n",
        "\n",
        "    predicted_label = xgb_clf.predict(sample_processed)[0]\n",
        "\n",
        "    probabilities = xgb_clf.predict_proba(sample_processed)[0]\n",
        "\n",
        "    print(\"Information:\")\n",
        "    print(sample_data, '\\n')\n",
        "    # print(f\"True class: {true_label}\")\n",
        "    if predicted_label == 0:\n",
        "        print(f\"This is not exoplanet\")\n",
        "    else:\n",
        "        print(f\"This is exoplanet\")\n",
        "\n",
        "    # print(f\"Probabilities: {probabilities}\")\n",
        "    print(f\"Class: {predicted_label}\", '\\n')\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "u4O-CnqrhTUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/new_combined.csv').dropna()\n",
        "loaded_model = joblib.load('/content/model_xgb_final.pkl')\n",
        "df = df.drop('disp',axis=1)\n",
        "\n",
        "features_num = [\n",
        "    'ra', 'dec', 'orbper', 'duration', 'depth', 'rad', 'steff',\n",
        "    'logg', 'srad'\n",
        "]\n",
        "features_cat = []\n",
        "\n",
        "transformer_num = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\"),\n",
        "    StandardScaler(),\n",
        ")\n",
        "\n",
        "transformer_cat = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\", fill_value=\"NA\"),\n",
        "    OneHotEncoder(handle_unknown='ignore'),\n",
        ")\n",
        "\n",
        "preprocessor = make_column_transformer(\n",
        "    (transformer_num, features_num),\n",
        "    (transformer_cat, features_cat),\n",
        ")\n",
        "\n",
        "preprocessor.fit(df)\n",
        "\n",
        "sample_idx = range(len(df))\n",
        "\n",
        "for i in sample_idx:\n",
        "    sample_data = df.iloc[i]\n",
        "\n",
        "    sample_processed = preprocessor.transform(pd.DataFrame(sample_data).T)\n",
        "\n",
        "    predicted_label = loaded_model.predict(sample_processed)[0]\n",
        "\n",
        "    probabilities = loaded_model.predict_proba(sample_processed)[0]\n",
        "\n",
        "    print(\"Information:\")\n",
        "    print(sample_data, '\\n')\n",
        "    if predicted_label == 0:\n",
        "        print(f\"This is not exoplanet\")\n",
        "    else:\n",
        "        print(f\"This is exoplanet\")\n",
        "\n",
        "    # print(f\"Probabilities: {probabilities}\")\n",
        "    print(f\"Class: {predicted_label}\", '\\n')\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "OFYYKmUQnkD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KNTiG08doMFN"
      }
    }
  ]
}